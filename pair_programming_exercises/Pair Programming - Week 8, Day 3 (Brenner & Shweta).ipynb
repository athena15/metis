{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "import re\n",
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = news.data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set network parameters\n",
    "window_size = 2\n",
    "dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def stem(doc, tkpat=re.compile('\\\\b[a-z][a-z][a-z][a-z]+\\\\b')):\n",
    "    return [stemmer.stem(token) for token in tkpat.findall(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(strip_accents='ascii',\n",
    "                                max_df=0.95, \n",
    "                                min_df=2,\n",
    "#                                 ngram_range=(5,5),\n",
    "#                                 stop_words='english',\n",
    "                                tokenizer=stem\n",
    "                               )\n",
    "tf = cv.fit_transform(corpus)\n",
    "\n",
    "vocab = cv.vocabulary_\n",
    "\n",
    "id2word = {v:k for (k,v) in vocab.items()}\n",
    "V= len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tf.toarray(), columns=cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wonder</th>\n",
       "      <th>anyon</th>\n",
       "      <th>there</th>\n",
       "      <th>could</th>\n",
       "      <th>enlighten</th>\n",
       "      <th>this</th>\n",
       "      <th>other</th>\n",
       "      <th>door</th>\n",
       "      <th>sport</th>\n",
       "      <th>look</th>\n",
       "      <th>...</th>\n",
       "      <th>embalm</th>\n",
       "      <th>anonym</th>\n",
       "      <th>righteous</th>\n",
       "      <th>beast</th>\n",
       "      <th>premis</th>\n",
       "      <th>hominem</th>\n",
       "      <th>stanley</th>\n",
       "      <th>calgari</th>\n",
       "      <th>faulti</th>\n",
       "      <th>circular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2908 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   wonder  anyon  there  could  enlighten  this  other  door  sport  look  \\\n",
       "0       0      0      0      0          0     0      0     0      0     0   \n",
       "1       0      0      0      0          0     0      0     0      0     0   \n",
       "2       0      0      0      0          0     0      1     0      0     0   \n",
       "3       0      0      0      0          0     0      1     0      0     0   \n",
       "4       0      0      0      0          0     0      0     0      0     0   \n",
       "\n",
       "     ...     embalm  anonym  righteous  beast  premis  hominem  stanley  \\\n",
       "0    ...          0       0          0      0       0        0        0   \n",
       "1    ...          0       0          0      0       0        1        0   \n",
       "2    ...          0       0          0      0       0        0        0   \n",
       "3    ...          0       0          0      0       0        0        0   \n",
       "4    ...          0       0          0      0       0        0        0   \n",
       "\n",
       "   calgari  faulti  circular  \n",
       "0        0       0         0  \n",
       "1        0       0         0  \n",
       "2        0       0         0  \n",
       "3        0       0         0  \n",
       "4        0       0         0  \n",
       "\n",
       "[5 rows x 2908 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['any help would be appreciated', 'that he is of the', 'would like to be able', 'like to be able to', 'left corner of the window', 'so it stands to reason', 'it stands to reason that', 'dod 8177 technician dr speed', '8177 technician dr speed not', 'technician dr speed not student', 'women children and elderly people', 'source documents volume 1919 document', 'documents volume 1919 document no', 'archive no 3671 cabin no', 'no 3671 cabin no 163', '3671 cabin no 163 drawer', 'cabin no 163 drawer no', 'no 163 drawer no file', '163 drawer no file no', 'last modified date 93 04', 'modified date 93 04 01', 'date 93 04 01 14', '93 04 01 14 39', 'does anyone out there know', 'information will be greatly appreciated', 'here is another one source', 'while at the same time', 'for nearly one thousand years', 'nearly one thousand years the', 'one thousand years the turkish', 'thousand years the turkish and', 'years the turkish and kurdish', 'the turkish and kurdish people', 'turkish and kurdish people lived', 'and kurdish people lived on', 'kurdish people lived on their', 'people lived on their homeland', 'lived on their homeland the', 'on their homeland the last', 'their homeland the last one', 'homeland the last one hundred', 'the last one hundred under', 'last one hundred under the', 'one hundred under the oppressive', 'hundred under the oppressive soviet', 'under the oppressive soviet and', 'the oppressive soviet and armenian', 'oppressive soviet and armenian occupation', 'soviet and armenian occupation the', 'and armenian occupation the persecutions', 'armenian occupation the persecutions culminated', 'occupation the persecutions culminated in', 'the persecutions culminated in 1914', 'persecutions culminated in 1914 the', 'culminated in 1914 the armenian', 'in 1914 the armenian government', '1914 the armenian government planned', 'the armenian government planned and', 'armenian government planned and carried', 'government planned and carried out', 'planned and carried out genocide', 'and carried out genocide against', 'carried out genocide against its', 'out genocide against its muslim', 'genocide against its muslim subjects', 'against its muslim subjects million', 'its muslim subjects million turks', 'muslim subjects million turks and', 'subjects million turks and kurds', 'million turks and kurds were', 'turks and kurds were murdered', 'and kurds were murdered and', 'kurds were murdered and the', 'were murdered and the remainder', 'murdered and the remainder driven', 'and the remainder driven out', 'the remainder driven out of', 'remainder driven out of their', 'driven out of their homeland', 'out of their homeland after', 'of their homeland after one', 'their homeland after one thousand', 'homeland after one thousand years', 'after one thousand years turkish', 'one thousand years turkish and', 'thousand years turkish and kurdish', 'years turkish and kurdish lands', 'turkish and kurdish lands were', 'and kurdish lands were empty', 'kurdish lands were empty of', 'lands were empty of turks', 'were empty of turks and', 'empty of turks and kurds', 'of turks and kurds the', 'turks and kurds the survivors', 'and kurds the survivors found', 'kurds the survivors found safe', 'the survivors found safe heaven', 'survivors found safe heaven in', 'found safe heaven in turkiye', 'safe heaven in turkiye today', 'heaven in turkiye today soviet', 'in turkiye today soviet armenian', 'turkiye today soviet armenian government', 'today soviet armenian government rejects', 'soviet armenian government rejects the', 'armenian government rejects the right', 'government rejects the right of', 'rejects the right of turks', 'the right of turks and', 'right of turks and kurds', 'of turks and kurds to', 'turks and kurds to return', 'and kurds to return to', 'kurds to return to their', 'to return to their muslim', 'return to their muslim lands', 'to their muslim lands occupied', 'their muslim lands occupied by', 'muslim lands occupied by soviet', 'lands occupied by soviet armenia', 'occupied by soviet armenia today', 'by soviet armenia today soviet', 'covers up the genocide perpetrated', 'up the genocide perpetrated by', 'the genocide perpetrated by its', 'genocide perpetrated by its predecessors', 'perpetrated by its predecessors and', 'by its predecessors and is', 'its predecessors and is therefore', 'predecessors and is therefore an', 'and is therefore an accessory', 'is therefore an accessory to', 'therefore an accessory to this', 'an accessory to this crime', 'accessory to this crime against', 'to this crime against humanity', 'this crime against humanity soviet', 'crime of genocide against the', 'of genocide against the muslims', 'genocide against the muslims by', 'against the muslims by admitting', 'the muslims by admitting to', 'muslims by admitting to the', 'by admitting to the crime', 'admitting to the crime and', 'to the crime and making', 'the crime and making reparations', 'crime and making reparations to', 'and making reparations to the', 'making reparations to the turks', 'reparations to the turks and', 'to the turks and kurds', 'genocide of million muslim people', 'is this the joke of', 'this the joke of the', 'the joke of the month', 'anyone out there have any', 'all five schools of law', 'please send me information on', 'does anyone have any idea', 'have any idea where could', 'any idea where could find', 'at the end of the', 'trying to figure out how', 'to figure out how to', 'the genocide of million muslim', 'kenneth simon dept of sociology', 'simon dept of sociology indiana', 'dept of sociology indiana university', 'of sociology indiana university internet', 'sociology indiana university internet kssimon', 'indiana university internet kssimon indiana', 'university internet kssimon indiana edu', 'internet kssimon indiana edu bitnet', 'kssimon indiana edu bitnet kssimon', 'indiana edu bitnet kssimon iubacs', 'gordon banks n3jxp skepticism is', 'banks n3jxp skepticism is the', 'n3jxp skepticism is the chastity', 'skepticism is the chastity of', 'is the chastity of the', 'the chastity of the intellect', 'chastity of the intellect and', 'of the intellect and geb', 'the intellect and geb cadre', 'intellect and geb cadre dsl', 'and geb cadre dsl pitt', 'geb cadre dsl pitt edu', 'cadre dsl pitt edu it', 'dsl pitt edu it is', 'pitt edu it is shameful', 'edu it is shameful to', 'it is shameful to surrender', 'is shameful to surrender it', 'shameful to surrender it too', 'to surrender it too soon', 'newsgroups sci med path news', 'sci med path news larc', 'med path news larc nasa', 'path news larc nasa gov', 'news larc nasa gov saimiri', 'larc nasa gov saimiri primate', 'nasa gov saimiri primate wisc', 'gov saimiri primate wisc edu', 'zaphod mps ohio state edu', 'if you want to find', 'not at all the same', 'at all the same thing', 'dan johnson and god said', 'johnson and god said jeeze', 'and god said jeeze this', 'god said jeeze this is', 'said jeeze this is dull', 'jeeze this is dull and', 'this is dull and it', 'is dull and it was', 'dull and it was dull', 'and it was dull genesis', 'but if you were to', 'want to find out why', 'to be able to do', 'in the uk it impossible', 'the uk it impossible to', 'uk it impossible to get', 'it impossible to get approval', 'impossible to get approval to', 'to get approval to attach', 'get approval to attach any', 'approval to attach any crypto', 'to attach any crypto device', 'attach any crypto device to', 'any crypto device to the', 'crypto device to the phone', 'device to the phone network', 'to the phone network anything', 'the phone network anything that', 'phone network anything that plugs', 'network anything that plugs in', 'anything that plugs in to', 'that plugs in to our', 'plugs in to our bt', 'in to our bt phone', 'to our bt phone sockets', 'our bt phone sockets must', 'bt phone sockets must be', 'phone sockets must be approved', 'sockets must be approved for', 'must be approved for some', 'be approved for some reason', 'approved for some reason crypto', 'for some reason crypto devices', 'some reason crypto devices just', 'reason crypto devices just never', 'crypto devices just never are', 'on the back of the', 'has nothing to do with', 'is it possible to do', 'it possible to do wheelie', 'possible to do wheelie on', 'to do wheelie on motorcycle', 'do wheelie on motorcycle with', 'wheelie on motorcycle with shaft', 'on motorcycle with shaft drive', 'love to hear from you', 'have anything to do with', 'come to think of it', 'want to be able to', 'is not the same as', 'it to make sure that', 'to make sure that it', 'tell them that they are', '__ jorg klinger gsxr1100 if', 'jorg klinger gsxr1100 if you', 'klinger gsxr1100 if you only', 'gsxr1100 if you only new', 'if you only new who', 'you only new who arch', 'only new who arch eng', 'new who arch eng services', 'who arch eng services lost', 'arch eng services lost horizons', 'eng services lost horizons cr500', 'services lost horizons cr500 think', 'lost horizons cr500 think am', 'horizons cr500 think am umanitoba', 'cr500 think am umanitoba man', 'think am umanitoba man ca', 'am umanitoba man ca the', 'umanitoba man ca the embalmer', 'man ca the embalmer it175', 'ca the embalmer it175 anonymous'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
